====
План
====
* Структура и цели этого доклада
* Введение в парсинг для овощей
    * Элементарные определения
    * Регулярные языки
    * Контекстно-независимые языки
    * Иерархия Хомского
    * Алгоритмы парсинга
        * Методы "сверху-вниз"
            * Рекурсивный спуск, LL(k), packrat/PEG
        * Методы "снизу-вверх"
            * SLR, LALR, LR(k)
        * LL vs LR
        * Алгоритмы, для произвольных CTF грамматик
            * GLR, CYK, Earley
    * Семантические действия. Атрибуты
    * Литература
* Обзор генераторов парсеров
    * Псевдо-классификация инструментов для создания парсеров
        * Генераторы парсеров из внешней грамматики
        * Генераторы парсеров из внутренней грамматики
        * Комбинаторы парсеров
    * Конкретные инструменты для парсинга
        * Генераторы из грамматик
            * ANTLR
            * yacc/bison
            * bisonc++
            * lemon
            * Happy
        * Генератор из внутренней грамматики
            * Pegged
            * PCRE
            * TODO: need more
        * Комбинаторы парсеров
            * boost::spirit
            * parsec
            * pyparse
    * Добро пожаловать в реальный мир
        * Генераторы парсеров в компиляторах
        * Трудности использования
        * Контекстная зависимость
        * Неодназначность грамматик
        * Языки со значимыми пробелами
        * Обработка ошибок
    * Литература
* Дополнительные темы
    * Функциональные парсеры
    * Сочетание комбинаторов и анализ
    * Layout-sensitivity
    * Инкрементальная генерация парсеров
    * Парсинг с использование производных
    * Fuzzy парсинг
    * Литература

=================================
0. Структура и цели этого доклада
=================================

Задачи этого доклада: WIP
Дать "карту" по различным инструментам для парсинга, показать основные сходства
и различия между различными инструментами и указать основные сильные и слабые
стороны как конкретных инструментов, так и общих подходов к парсингу.

В этом докладе рассматривается только парсинг искусственных языков. Под
искусственными языками понимаются языки программирования, конфигураций и прочих
DSL, предназначенных для обработки компьютерами. В противоположность им,
естественными языками занимается вычислительная лингвистика, и хотя большая
часть теории формальных языков применима в обоих случаях, вычислительная
лингвистика обычно требует специфичных для своей предметной области методов
разбора.

Доклад разделён на три части. Первая часть служит неформальным обзором теории
формальных языков. Её цель — напомнить терминологию и различные алгоритмы,
используемые при парсинге. Вторая часть рассматривает наиболее часто
используемые генераторы парсеров, как конкретные случаи, так и в общем и
проблемы, связанные с ними. Третья часть рассматривает некоторые дополнительные
вопросы, которые менее известны, но, тем не менее, являются достаточно
интересными. 

================================
1. Введение в парсинг для овощей
================================

Если Вам хотя приблизительно известен смысл всех этих терминов, то эту часть
можно смело пропустить:
* Иерархия Хомского
* Алгоритмы: LL(k), LL, LL(*), GLL, LR(k), SLR, LALR(k), GLR, packrat/PEG,
    рекурсивный спуск, CYK, Earley
* LR: shift/reduce
* Синтезируемые и наследуемые атрибуты

Элементарные определения
------------------------
    Под алфавитом ∑ понимается любое (но обычно непустое) конечное множество.
    Примеры: алфавит из двух букв: ∑ = {a, b}, алфавит русских букв.

    Операция _* над множеством определяется как X* = ⋃ Xⁱ по всем i ∈ ℕ₀.
    Под множеством в степени i понимается обычное декартово произведение i раз.
    (Side note: кстати, уже здесь можно видеть проблемы наивной теории множеств:
    операция _* действует над классом всех множеств.)

    Словами называются элементы множества ∑*. Языком называется подмножество
    L ⊂ ∑*. Пустое слово будем в дальнейшем обозначать ε.

    Языки являются множествами, поэтому для них также определены операции
    объединение L₁ ⋃ L₂, произведение: L₁⋅L₂ и звезда Клини: L*.

    Замечание: в этом докладе везде произведение считается ассоциативным, т.е.
    элементами множества (A ⋅ B) ⋅ C являются всевозможные тройки (a, b, c), а
    не вложенные кортежи: ((a, b), c), если бы мы следовали дословно
    стандартному определению. То же самое распространяется и на звезду Клини.

Регулярные языки
----------------
    Классом регулярных языков над алфавитом ∑ = {a, b, c, ... }  называется
    минимальный класс, содержащий языки:
    ∅, {ε}, {a}, {b}, {c}, ... 
    и замкнутый относительно операций объединения, умножения и звезды Клини.

    Регулярные языки естественным образом описываются с помощью регулярных
    выражений (приоритет операций: '*', '⋅', '|', круглые скобки используются
    для группировки). Например:
    * abc|a                 L = {abc, a}
    * ab*                   L = {a, ab, abb, abbb, ...}
    * [a|b(,a|b)*]          L = {[a], [b], [a,a], [a,b], [b, a], ...}
    * (0|11|10(1|00)*01)*   L = <числа в двоичной записи, кратные 3>
    Через стандартные операции объединения, умножения и звезды Клини можно
    выразить также дополнительные операции ? (0 или 1 повтор), {n} (точно n
    повторов), {n, m} (от n до m повторов).

    На самом деле класс регулярных языков и тех, которые распознают автоматы
    совпадают. Есть простой алгоритм, превращающий автомат с n состояниями, из
    которых |A| принимающих в выражение размером O(n⋅|A|⋅|∑|). Но конверсия в
    обратную сторону: из регулярного выражения в автомат может быть
    экспонециально большой.

    Пример: конечный автомат, распознающий числа, кратные трём:

       0                           1    
      +--+                        +--+  
      |  |                        |  |  
      |  |                        |  |  
    +----v-+  1   +------+  0   +----v-+
    |      +------>      +------>      |
    | 3k+0 |      | 3k+1 |      | 3k+2 |
    |      <------+      <------+      |
    +------+  1   +------+  0   +------+

    Чтобы доказать, что какой-либо язык не является регулярным, обычно можно,
    если понять, что конечного количества состояний в автомате не хватит чтобы
    его распознать. Например язык правильных скобочных последовательностей не
    является регулярным, потому что если, допустим в распознающем автомате n
    состояний, то после прочтения строчки (ⁿ⁺¹ автомат перейдёт в какое-то из
    предыдущих состояний, когда он ещё только прочёл строчку (ⁱ. Значит он не
    сможет различить строчки (ⁿ⁺¹ )ⁱ и (ⁱ)ⁱ, а значит не может распознать язык.
    Ещё для этих же целей можно использовать лемму о накачке,
    но она обычно не так интуитивна.

    [Note] формулируется она так: пусть L — регулярный язык. Для него есть
    такой n, что все слова α длинной больше n можно разрезать на 3 части: x y z,
    α=xyz (причём часть y не пустая), такие что все слова xyⁱz тоже будут
    входить в язык. 

    Используя автомат, легко показать, что дополнение и пересечение регулярных
    языков регулярно.

Контекстно-независимые языки
----------------------------
    На языках уже определена арифметика с использованием операций умножения,
    объединения и звезды Клини. Из этих выражений можно составлять уравнения,
    например: N = {ε} ⋃ {'a'} ⋅ N ⋅ {'b'}. Решением этого уравнения является
    только язык {aⁿbⁿ | n ∈ ℕ₀}. Система из одного или нескольких таких
    уравнений задаёт язык.
    Если набор уравнений такой, что каждый язык стоит сам по себе в левой части
    какого-нибудь уравнения ровно по одному разу и других уравнений нет, то эта 
    система уравнений называется контекстно-свободной граматикой, а её
    минимальное решение (нетрудно показать, что такое существует), называется
    контекстно-свободным языком.

    Например грамматика правильных скобочных последовательностей:
    S = S S | '(' S ')' | ε. Минимальное решение это все правильные скобочные
    последовательности.  Максимальное, это ∑*.

    Автомат со стеком может распознавать контекстно-свободные языки и только их.

    Примеры языков не являющиеся контекстно свободными: {aⁿbⁿcⁿ| n ∈ ℕ},
    {ww| w ∈ ∑*}.
    
    Для контекстно-независимых языков тоже есть лемма о накачке.
    Пусть L — какой-то контекстно-свободный язык. Тогда есть n, такой что
    все слова α ∈ L длинной больше n можно разрезать на 5 частей: u, x, v, y, w,
    (причём части x и y не пустые): α = uxvyw, причём слова такие, что все слова
    uxⁱvyⁱw также находятся в языке L.

Иерархия Хомского
-----------------
    Иерархия Хомского это такая классификация языков, по тому, насколько просто
    мы умеем их распознавать. Выделяет 4 типа: регулярные ⊂ контекстно-свободные
    ⊂ контекстно-зависимые ⊂ неограниченные. В теории обычно упор делается на
    методы разбора контекстно-свободных языков. Однако на практике очень часто
    встречаются языки, которые уже не являются контекстно-свободными, хотя
    почти всегда для них есть методы, не требующие обращения со всеми
    контекстно-зависимыми грамматиками. В этом докладе мы будем находиться
    где-то на "стыке" контекстно свободных и контекстно-зависимых языков.

Алгоритмы парсинга
------------------
    Теперь мы рассмотрим методы разбора контекстно-свободных языков. На практике
    этот класс языков встречается наиболее часто (а те языки, которые в него не
    входят, на практике обычно в некотором смысле довольно близки к контекстно 
    свободным). Асимптотически лучший известный алгоритм распознавания языка
    по произвольной контекстно-свободной грамматике работает за O(n²⁺ᵉ), где e
    какая-то константа большая нуля, зависит от текущего года и соответствует
    лучшему алгоритму перемножения матриц, известному на данный момент. На
    практике, однако обычно используют ограниченный класс языков, для которых
    есть грамматики, которые можно распознавать в среднем за линейное время на
    практических случаях.


    — I wanna know what type of parser is used in the javac compiler. whether a 
    top-down or bottom-up parser is used..... ?
    — I think it's a top-down parser. It starts at the top of the program which
    is line 1 right?

    ### Методы "сверху-вниз" ###
        Под парсингом "сверху-вниз" подразумевается определённый порядок
        построения дерева. Вершины при этом появляются сверху-вниз и слева-
        направо.
        Например, вершины дерева:
                 1
            2          3
         4    5     6     7
        8 9 10 11 12 13 14 15
        Будут появляться в этом порядке: 1, 2, 4, 8, 9, 5, 10, 11, 3, 6, 12, 13,
        7, 14, 15.

        Рекурсивный спуск это один из наиболее реализаций парсинга сверху-вниз.
        Есть две разновидности: 1) предсказывающий, когда парсер, чтобы выбрать
        следующее ветвление использует следующие k символов. Это не отличается
        от LL(k) алгоритма. 2) пробующий по очереди варианты, пока хотя бы одно
        из них не сработает и использующий backtracking. Это не отличается от
        PEG парсера.

        1ый вариант работает за линейное время, но требует специальной
        грамматики. Второй потенциально может работать экспонециальное время.
        Оба варианта зависнут, если грамматика содержит левую рекурсию, хотя
        некоторые генераторы парсеров пытаются с этим бороться.

        Разновидность первого варианта: LL(*) — это парсер который смотрит,
        принадлежат ли символы впереди какому-то регулярному выражению.

        Разновидность второго варианта: packrat парсеры, которые используют
        дополнительную память, чтобы запоминать возможные варианты разбора
        строки, и за счёт этого гарантированно работают за линейное время. 
        (Мне кажется правда, что время должно быть умножено на количество
        терминалов+нетерминалов, не уверен).

    ### Методы "снизу-вверх" ###
        Под парсингом "сниз-вверх" понимают порядок построения дерева снизу-
        вверх и слева-направо.
        Например, вершины дерева:
                 1
            2          3
         4    5     6     7
        8 9 10 11 12 13 14 15
        Будут появляться в этом порядке: 8, 9, 4, 10, 11, 5, 2, 12, 13, 6, 14,
        15, 7, 3, 1.

        Парсеры строят дерево используя всего две операции:
        shift() и reduce(Rule). (Дальнейшее нужно объяснять на картинке) 
        shift добавляет новый лист дерева на стек, reduce(r) берёт последние k
        вершин на стеке, соответствующие правой части правила r убирает их со
        стека, подвешивает к новой вершине-нетерминалу, и кладёт новую вершину
        на стек. 

        Есть несколько алгоритмов, которые это делают.
        LR(0) — использует автомат и только информацию на стеке, чтобы решить
        между shift и reduce. Очень ограничен в применении.
        SLR — примитивное улчшение LR(0), заключающееся в том, что он использует
        множества FOLLOW для нетерминального символа, чтобы решить, нужно ли
        делать по нему reduce. Содержит в точности то же самое количество
        состояний, что и SLR.
        LR(1) — для каждого отдельного случая явно вычисляет множество FOLLOW,
        и это множество становится частью состояния. Из-за этого очень сильно
        вырастает размер таблицы, но получившийся парсер парсит большое
        подмножество практически интересных грамматик.
        LALR(1) — простой способ уменьшения размеры таблицы LR(1) парсера за 
        счёт сливания в одно состояний, которые отличаются только FOLLOW
        множествами. Получается какой-то компромис между SLR и LR(1).

        LR(k), LALR(k) — обобщение предыдущих алгоритмов на k символов.

        Рекурсивный подъём — способ релизации любого из алгоритмов из LR
        семейства, но в качестве стека состояний используется рекурсивный вызов
        функции. Потенциально может работать быстрее. Или медленнее.

        Все эти алгоритмы работают за O(n)

    ### LL vs LR ###
        Включения языков:
        LL(k) ⊂ LR(k), но LL(*) не сравнимо с LR.

        LL грамматики не любят левую рекурсию. LR любят левую рекурсию, и
        недолюбливат правую (они её принимают, но растёт стек).

        Таблицы для LR парсеров и количество состояний как правило больше, чем
        у LL.

        LL парсеры обычно без проблем поддерживают синтезируемые и наследуемые
        атрибуты. Для LR парсеры, как правило довольствуются лишь синтезируемми
        атрибутами.

    ### Алгоритмы, для произвольных CTF грамматик ###
        CYK — динамика для грамматик в нормальной форме. Работает за O(n³).
        
        GLR — недетерменированное выполнение LR алгоритма с объединением
        различных ветвей, как оптимизация. Работает O(n³) в худшем случае, но
        за O(n) на LR(1) грамматиках, не содержащих конфликтов и за O(n²) в
        однозначных грамматиках.
        
        Earley — TODO

Семантические действия. Атрибуты
--------------------------------
    Теория формальных языков обычно обеспокоена вопросом о том, принадлежит ли
    заданное слово языку. Однако на практике людей обычно интересует какая-то
    информация, связанная с разбором строки, например синтаксическое дерево.
    Обычно это реализуют через семантические действия, которые для парсера
    являются просто нетерминалами, которые генериуют ε. При этом, если эти
    нетерминалы оказываются в разборе то выполняется соответствующий код для
    них. [TODO: плохо сформулировал].

    Переменные, которые привязаны к грамматическому правилу называют атрибутами.
    Атрибуты, разделяют на синтезируемые и наследуемые.
    Наследуемые, это аналог аргументов функции: они вычисляются в вышележащих
    узлах синтаксического дерева. Синтезируемые — аналог возвращаемых значений:
    они вычисляются через атрибуты узлов ниже по синтаксическому дереву.

Литература
----------
    Стандартная отсылка, книго о построении компиляторов:
    * «Compilers: Principles, Techniques, and Tools», авторы Alfred V. Aho,
    Monica S. Lam, Ravi Sethi, Jeffrey D. Ullman.
    
    Стендфордские lecture notes к cs143 compilers.:
    * http://web.stanford.edu/class/archive/cs/cs143/cs143.1128/

    Описано много алгоритмов парсинга, при этом идёт упор на наглядность и
    доступность изложения.
    * «Parsing techniques. A practical guide», 2d edition авторы Dick Grune,
    Ceriel J.H.Jacobs

    Статьи
    * «Packrat Parsers Can Support Left Recursion», 2008,
        Alessandro Warth, James R. Douglass, Todd Millstein
    * «LL(*): The Foundation of the ANTLR Parser Generator», 2011,
        Terence Parr, Kathleen S. Fisher
    * «GLL Parsing», 2010, Elizabeth Scott, Adrian Johnstone
    * «General context-free recognition in less than cubic time», 1974,
        Leslie Valiant
    * «Recursive Ascent: An LR Analog to Recursive Descent», 1988,
        George H. Roberts

=============================
2. Обзор генераторов парсеров
=============================

Псевдо-классификация инструментов для создания парсеров
-------------------------------------------------------
    Есть приблизительно три основных способа, как задать парсер.
    1. Объект, явно содержащий грамматику выраженный в языке программирования.

    Основной минус этого подхода в том, что обычно нельзя встроить семантические
    действия в грамматику, а приходится идти через какое-то промежуточное
    представление. Хотя в языках динамических языках типа python или
    языках, это должно быть решаемой проблемой без каких-то сильных ухищрений.
    В D с хорошей поддержкой compile-time вычислений это тоже не должно быть
    проблемой. Ещё вроде-бы можно описывать структурно грамматику и вставлять
    лямбды в качестве семантических действий, но так, видимо, почти никто не
    делает.

    Основные плюсы этого подхода в том, что такой парсер обычно очень легко
    использовать (просто подключить библиотеку) и в том, что можно использовать
    язык программирования в качестве, макросов чтобы задавать грамматику.

    2. Комбинаторы парсеров.

    Основные плюсы, это:
    * Возможность использовать язык программирования, чтобы создавать новые
    комбинаторы
    * Бесплатная проверка типов
    * Легко встраивать семантические действия и сразу считать то, что необходимо
    * В большинстве библиотек есть возможность парсить не только
    контекстно-свободные языки

    Основные минусы:
    * Все комбинаторы парсеров по сути реализуют один и тот же алгоритм —
    рекурсивный спуск, а он не поддерживает левую рекурсию
    * Парсеры обычно вынуждены использовать backtracking (хотя вроде есть breadth first парсеры)
    * За комбинаторы парсеров часто может стать визуально плохо видна структура
    грамматики из-за использования многочисленных операторов и синтаксических
    изощрений
    * Следствие: такую грамматику трудно переносить на другой язык

    3. Генератор парсеров.

    Основные плюсы:
    * Может реализовать сколь угодно эффективные алгоритмы парсинга
    * Обычно нет проблем с семантическими действиями
    Основные минусы:
    * Нельзя использовать язык, чтобы определять "макросы" для упрощения
    написания синтаксиса
    * (Незначительный) приходится изучать новый синтаксис написания
    грамматических правил
    * (В зависимости от генератора) приходится работать с дополнительными
    странностями и ограничениями ещё одной программы.

Конкретные инструменты для парсинга
-----------------------------------
    [TODO: BNF, EBNF, ABNF, PEG]
    [TODO: это черновой вариант] 
    Генераторы парсеров

    ### ANTLR ###

        * EBNF синтаксис
        * LL(*) с улучшениями (умеет в некоторых случаях удалять левую рекурсию)
        * IDE plugins (Itellij, netbeans, eclipse, visual studioetc.)
        * BSD License
        * Используется в Groovy, Jython, IntelliJ IDEA, Twitter
        * Совместный лексер и парсер
        * v4 Генерирует код в Java, C#, Python, Javascript (предыдущие версии
            поддерживали больше)
        Возможности:
            * Принимает любую грамматику кроме содержащих неявную левую
            рекурсию. Умеет удалять непосредственную левую рекурсию 
            * Визуализатор дерева
            * Может парсить некоторые context-sensistive языки, а также
            разрешать с помощью них неоднозначности в грамматике с помощью
            semantic predicates. Но я не уверен, что это всегда можно.
                http://meri-stuff.blogspot.com/2012/12/antlr-semantic-predicates.html
            * Поддерживает нежадные версии ?, *, +, а именно ??, *?, +?
            * Хорошая поддержка атрибутов : наследуемые и синтезируемые (не
            отменяет всякие хаки с динамическим scope [TODO: что я имел в ввиду?])
            знает об ассоциативности операторов (по умолчанию влево)
            можно настраивать обработку ошибок
            * Встроенный лексер.
        [TODO: fuzzy parsing (channel?)?]
        Проблемы:
            * В документации периодически приходится угадывать о том, как вещи
            работают из примеров. Кроме того документация очень часто (примерно
            на каждой странице сайта antlr) ссылается на книгу, электронная
            копия которой стоит >=27$
        Резюме:
            Очень хороший генератор парсеров. Использовать очень просто и удобно.
            Если подходит под ваши цели и умеет генерировать в ваш язык, то дальше
            и смотреть не нужно.

    ### yacc/bison ###
        Моё мнение не беспристрастно из-за того, что я пытаюсь использовать этот генератор парсеров для С++
        GPLv3
        LALR / GLR, экспериментально: IELR (никто об этом почти не слышал), LR(1)
        обратно совместим с грамматиками yacc
        yacc написан 1970, bison 1988
        BNF грамматика
        генерирует в C, C++, Java
        Используется в Ruby, PHP, Go, MySQL, Bash
        Лексер обычно отдаётся для lex/flex
        Возможности:
            GLR парсер фактически позволяет работать с любой ctf грамматикой (хотя GLR, который реализован в bison потенциально может иметь экспонециальную сложность, хотя в теории мог бы иметь в худшем случае кубическую)
            Предикаты позволяют контролировать исполнение кода. Они более общие, чем ANTLR (который лишь ограниченно может их использовать для принятия решения, необх. но не достаточное условие, чтобы они были в начале ветвления. Иначе он просто кинет exception и попробует допарсить грамматику, как будто ничего не было)
        Проблемы:
            синтаксис не самый приятный. Например использовать $$ и $i в качестве имён переменных это позапрошлый век. Пардон, в новых версиях он поддерживает именованные аргументы. Синтаксис не консистентный и порой бывают действительно "магические инструкции" пример?
            Если не использовать GLR парсер(?), shift-reduce и reduce-reduce конфликты трудно понимать (нужно разбираться в работе алгоритма)
            Приходится писать много вспомогательного кода прямо в грамматике, просто чтобы парсер начал что-то делать
                Например, нужно писать прологи, прологов может быть несколько, они могут перемешиваться с какими-то объявлениями
                и зависеть друг от друга. Выглядит всё это как постоянное жонглирование какими-то хаками.
            Приходится использовать препроцессор. 
            Ориентирован на C, это сказывается.
            Грамматика всегда оказывается очень сильно связанной с кодом.
            Документация слишком длинная. Огромная её часть говорит о всяких архаизмах =(
            Чтобы сделать чистый парсер без сайдэффектов и засорения global namespace приходится возиться с кучей опций
                http://esr.ibiblio.org/?p=6341
            Скорей всего это чистый ад заставить работать несколько парсеров одновременно в одном проекте. Приходится возиться с опциями, которые дописывают префиксы ко всем функциям!! По умолчанию это yy. Следующий по популярности, это zz. Это пиздец.
            Как сделать локальные переменные для правила?
            Объявление типа синтезируемых атрибутов правил отделено собственно от места, где оно используется а-ля паскаль.
            С типом атрибутов полная каша. Во-первых существует около 5 несовместимых друг с другом способов объявления типов атрибутов. Далее bison не поддерживает нормальную типизацию, и тип на самом деле только один. Поэтому он просто собирает все типы в большое жирное объединение, а это означает потенциальные проблемы с проверкой типов. Причём даже фича, которая сама собирает типы в жирное объединение появилась недавно и не совместима с yacc. В новых версиях есть поддержка variant, что делает ситуацию немного лучше.
            Как сделать различные семантические возвращаемые значения для правила?
            Поддерживает наследуемые атрибуты, но совершенно безобразным образом: приходится индексироваться назад по стеку LR парсера. Это не безопасно, и нарушает идеи инкапсуляции со всеми вытекающими проблемами.
            По умолчанию генерирует LALR. Иногда это может создавать непонятные проблемы (см. "5.7 Mysterious Conflicts"), которые даже трудно идентифицирровать. Решение: использовать мало кому известный IELR (о нём похоже есть всего пара статьей и никакой информации в интернете) или LR(1) или GLR
            Нет стандартного способа визуализировать дерево разбора. Есть способ визуализировать парсер, но это обычно не практично
            Генерируемые файлы от flex огромны. Вместо того, чтобы подключать общую часть, они всё вываливают на месте. Из-за этого их сложней понимать.
            Грамматика сильно связана с конкретным кодом, который её использует. Использовать её в разных языках без модификации на грани невозомжного.
            Заставить работать flex по-нормальному с с++ довольно трудно. Как признаются сами разработчики в одном из комментариев:
                    /* The c++ scanner is a mess. The FlexLexer.h header file relies on the
                     * following macro. This is required in order to pass the c++-multiple-scanners
                     * test in the regression suite. We get reports that it breaks inheritance.
                     * We will address this in a future release of flex, or omit the C++ scanner
                     * altogether.
                     */
            Нельзя вернуть семантическое значение после парсинга. Единственный способ: использовать какой-то state и хранить результат в нём.
            Обработка ошибок сложнее, чем в ANTLR и несколько работает. Более того, её сложнее настроить (у flex она не работает из коробки, и требует установки каких-то криптографических макросов)
            Семантический предикаты почему генерируют неправильный код с моими опциями.

    Резюме
        Если вы из любителей постоять в гамаке на одной ноге в акваланге, то этот генератор парсеров для вас.

    ### bisonc++ ###
        [TODO]

    ### Lemon ###
        Написан для sqlite
        LALR(1)
        BNF
        re-entrant и thread-safe по умолчанию, в отличии от bison
        Генерирует код толькко в C
        лицензия Public Domain
        Если писать под встроенные системы, то возможно, это очень хороший вариант
        Генерирует мало кода, причём то что генерирует, неплохо документировано
        Возможно, bison будет работать быстрей, однако
        Проще дизайн, чем у bison
        Крошечная документация
        Bison вызывает сам тащит токены (pull), здесь наоборот (push) (это приятней, можно, например писать парсер, работающий на бесконечных stream-ах)
        Из-за этого также легче тестировать
        Нет проблем использованием глобальных переменных
        Так как парсер никак не использует global state, для него очень просто написать обёртку для C++.
        Токенайзер в него легче встроить
        Проблемы:
            Генерирует код только в C
            Ограничен исключительно LALR(1). Нет дополнительных фич.
            Естественно, никаких вам variant-ов.
            И вообще почти никаких дополнительных фишек.
            Наследует почти все проблемы от bison
            Только синтезируемые атрибуты.
        Подводный камень:
            1. Генератор не генерирует объявления функций. Их нужно написать самому.
            2. Стандартный шаблон почему-то не подключает assert.h, хотя повсюду использует assert. (нужно подключить его самому)
            3. Скорей всего он не поставляется как пакет для дистрибутива. Поэтому его придётся несколько вручную подключать к проекто. Но по сути весь генератор парсеров, это всего один файл и его можно поставлять вместе с проектом.
        Резюме
            Если нужно писать на чистом C (особенно для встроенных систем), и важно reentrancy и нужно только что-то очень простое то может быть он и сойдёт. Плюс: его довольно легко изучить (из-за документации). Bison и Lemon довольно близко друг отдруга стоят. Если бы мне ничего специфичного не нужно было бы, и bison не был бы в моей оперативной памяти со всеми его особенностями, то я наверное предпочёл бы Lemon.

            Small size of generated code and memory footprint. It produces the smallest parser I found (I compared parsers of similar complexity generated by flex, bison, ANTLR, and Lemon);
            Excellent support of embedded systems: Lemon doesn't depend on standard library, you can specify external memory management functions, debug logging is removable.
            Public domain license. There is separate fork of Lemon licensed under GPLv2 that is not suitable for our needs because of viral license. So we get latest sqlite sources and compile Lemon out of them (it consists of only two files);
            Pull-parsing. It makes code more straightforward to understand and maintain than Flex/Bison parsing code. Thread-safety as an additional bonus I admire.
            Simple integration with tokenizers. Our project nature requires tokenizing of binary stream with variable tokens size. It was quite an easy to implemented tokenizer and integrate with parser API of only 3 functions and one feedback context variable. We investigated ways of integrating Lemon with re2c and Ragel and found them also quite easy to implement.
            Very simple syntax fast to learn.
            Lemon explicitly separate development of tokenizer and lexical analyzer(parser). My development flow starts with designing of parser grammar. I'm able to check complex rules with implicit token sequence by the means of several Parser(...) calls at this first stage. Tokenizer is implemented afterwards.

    ### Happy ###
        BNF
        используется, например в Agda и GHC, наверное много где ещё.
        язык haskell
        копирует релевантный синтаксис от yacc/bison 
        LALR, GLR
        использует по-прежнему нумерацию токенов
        Очень просто использовать
        Хорошая документация
        семантических предикатов нет (вроде бы).


    Явный грамматический объект

    ### Pegged ###
        Boost License
        D язык, compile-time генрация кода
        recursive descend
        уже не развивается. Но если вы решили попробовать писать на D, то вы в любом любитель приключений и неизведанных территорий.
        Умеет грамматики, семантические действия
        Очень прост в использовании.
        Однако всегда генерирует полное AST (не нужно)
        Интересен просто как концепция.

        Резюме
            Если вы используете D, вы и так можете решить нужно вам это или нет.

    ### PCRE ###
        pcre != perl
        не буду сильно углубляться. есть много различных диалектов, и разные множества поддерживаемых фич.
        В том или ином виде поддерживается всеми
        Очень много диалектов и различий во всём.
        скорей всего PEG
        вот вам огромная таблица: https://en.wikipedia.org/wiki/Comparison_of_regular_expression_engines
        Не поддерживают ни семантических действий, ни атрибутов.
        Синтаксис не самый лакончиный, хотя, если скосить глаза, то читабельныый
        Поддерживают только текст. В зависимости от имплементации поддерживают только конкретный набор кодировок.
        Приходится вручную прописывать пробелы. Это легче, чем может показаться. Возможно есть способ хорошо это автоматизировать.
        флаг X позволит писать комментарии и форматировать выражение
        В зависимости от имплементации могут быть проблемы с восстановлением ast.
        Фактически в рантайме генерирует парсер.
        Как ни странно, писать грамматику не труднее классического варианта.
        Чтобы применить нужно только учить эпсилон синтаксиса, если знаком с обычными выражениями.
        С бэкреференсами - NP полны. Пример, классический 3-CNF SAT:
               (!$a ||  $b ||  $d)
            && ( $a || !$c ||  $d)
            && ( $a || !$b || !$d)
            && ( $b || !$c || !$d)
            && (!$a ||  $c || !$d)
            && ( $a ||  $b ||  $c)
            && (!$a || !$b || !$c)

            Трансформируем:
            $regex = '/^
                (x?)(x?)(x?)(x?) .* ;
                (?: x\1 | \2  | \4  ),
                (?: \1  | x\3 | \4  ),
                (?: \1  | x\2 | x\4 ),
                (?: \2  | x\3 | x\4 ),
                (?: x\1 | \3  | x\4 ),
                (?: \1  | \2  | \3  ),
                (?: x\1 | x\2 | x\3 ),
            $/x';

            и тестируем на : 'xxxx;x,x,x,x,x,x,x,'

            Если получим match, то значит она разрешима:
                array(5) {
                  [0]=> string(19) "xxxx;x,x,x,x,x,x,x,"
                  [1]=> string(1) "x"
                  [2]=> string(1) "x"
                  [3]=> string(0) ""
                  [4]=> string(0) ""
                }
                тогда:  $a = true, $b = true, $c = false and $d = false
        Резюме:
            Если имплементация нормально поддерживает рекурсивный матчинг, то на ней
            можно писать простенькие парсеры. По сути PRCE это генератор парсеров в
            рантайме с немного неудобным синтаксисом. На их счёт есть много предрассудков.
            Большие парсеры может оказаться генерировать не удобно. Довольно искусственно ограниченны в использовании.

            Ещё интенсивное использование регекспом это весёлый способ обратить всех, кто читает твой код против себя.


    Комбинаторы парсеров

    ### boost::spirit ###
        Что люди только не придумают, лишь бы не писать на хаскеле
        Boost license (близка к MIT license)
        LL, рекурсивный спуск
        PEG
        Язык, естественно только C++
        dsl строится на C++ шаблонах
        парсер (Qi), лексер(Lex), принтер(Karma)
        По сути комбинаторы парсеров на плюсовых темплейтах, аналогично Parsec
            отсюда плюсы и минусы:
                минусы: не поддерживается предпроцессинг, приходится вручную задавать
                ассоциативность операторов и порядок операторов через правила в грамматике, не поддерживается левая рекурсия.
                плюсы: dsl встроен в C++, легко смешивать семантику и грамматику (динамические парсеры), можно писать не только ctf грамматики, есть проверка типов, нормальные атрибуты (наверное), можно надеется на оптимизации со стороны компилятора. Теоретически должен генерировать довольно быстрые парсеры. Можно повышать уровень абстракции, делать свои комбинаторы из существующих.
        Так много шаблонов, то стандартные проблемы: увеличивается размер кода и время компиляции (и память требуемая для компиляции. Её может даже случайно не хватить.) (правда ли это на самом деле??)
        синтезируемые, наследуемые атрибуты
        Почти не используют для парсинга настоящих языков программирования, хотя есть контрпримеры. Часто используют для парсинга всяких DSL-ей
        Хорошая документация
        В семантических действиях используются индексы для указания нужного нетерминала. Плохо. (может есть способ именования переменных?)
        использует backtracking
        Внутреннюю машинерию может быть трудно понять.
        Язык dsl явно не минимальный. Нужно писать некоторое количество дополнительных объявлений. Это ограничения, накладываемые C++.
        Время комплияции огромно. Компляция небольшого примерчика xml парсера занимает 16 секунд и потребляет почти гигабайт оперативной памяти.
        Легко включить дебаг прямо в рантайме
        Резюме
            Концептуально вполне неплохой генератор парсеров, однако практически не подходит для работы над чем либо, что требует частого модифицирования парсера. Большое время компиляции скажется на трудности в отладке и вообще в использовании. Для каких-то небольших языков конфигов, наверное, впрочем, его ещё можно использовать. 

    ### parsec ###
        BSDv3
        Писать парсер несколько сложней, чем просто генерировать из грамматики
        Можно создавать свои комбинаторы парсеров, и управлять правилами на более абстрактном уровне
        Полностью поддерживает любые атрибуты, а также стейт
        рекурсивный спуск, PEG
        сильно языко-зависим
        Монады.
        Хорошая обработка ошибок.
        Туча вариаций.
        Грамматику за кодом программы может быть сложно увидеть

        megaparsec
        uu-parsinglib
        attoparsec
        polyparse

    ### pyparse ###
        [TODO]

Добро пожаловать в реальный мир
-------------------------------
    Clang, Perl, GNU компиляторы (C (gcc), C++ (g++), Objective-C, 
        Objective-C++, Fortran (gfortran), Java (gcj), Ada (GNAT), Go (gccgo),
        Pascal (gpc),... Mercury, Modula-2, Modula-3, PL/I, D (gdc), and VHDL
        (ghdl)), вроде бы javac (LL(1), recursive descent)
    не используют генератор парсеров, а вместо этого используют что-то
    написанное вручную. Однако по крайней мере Haskell, Ruby, Agda, живут на
    генераторах парсеров/
    Комментарий в http://gcc.gnu.org/wiki/New_C_Parser:
        Benefits:

        Although timings showed a 1.5% speedup, the main benefits are facilitating of future enhancements including: OpenMP pragma support; lexing up front for C so reducing the number of different code paths; diagnostic location improvements (and potentially other diagnostic improvements); merging cc1/cc1obj into a single executable with runtime conditionals (which has been of interest to some Apple people in the past). Many defects and oddities in the existing parser which would not have been readily fixable there have been identified, recorded with ??? comments in the new parser and reproduced bug-compatibly and the new parser will facilitate their fixing. 

    Проблемы, возникающие, при использовании генераторов парсеров:
    * контекстная зависимость (напр Agda с mixfix операторами, C++ parsing undecidable,
        Haskell с задаваемой ассоциативностью и приоритетом операторов, сетевые протоколы: <length>:<content>)
             Что делать?
                * Даже генераторы контекстно-независимых парсеров иногда ограниченно поддерживают контекстно-зависимость
                * 2-ух уровневые грамматики?
                * Вручную писать парсер и делать ужасные велосипеды
                * Монадические и апликативные(?) парсеры поддерживают такие дела https://byorgey.wordpress.com/2012/01/05/parsing-context-sensitive-languages-with-applicative/
                * Есть какие-либо генераторы?
                * Сделать парсинг многопроходовым (не сработает с обычными генераторами парсеров)
    * неоднозначные грамматики
        * generalized парсеры. GLR/GLL. Возможно, Earley.
    * вывод ошибок
    * WYSIWYG vs WYSIWYGIYULR(k)
    * Bonus: fuzzy parsers (просто упомянуть)

Литература
----------
    [TODO]

======================
3. Дополнительные темы
======================

Маленькие кусочки. Не могу дать полноценную лекцию по каждой теме.
Цель кругозор.

Layout-sensitivity
------------------
    * Контекстно-зависмый парсер
    * Анти-пример ghc
    * Используется как минимум в Haskell, Ocaml, Python, Coffescript, Markdown, hamlet languages, YAML
        Hamlet (HTML)

        $doctype 5
        <html>
            <head>
                <title>#{pageTitle} - My Site
                <link rel=stylesheet href=@{Stylesheet}>
            <body>
                <h1 .page-title>#{pageTitle}
                <p>Here is a list of your friends:
                $if null friends
                    <p>Sorry, I lied, you don't have any friends.
                $else
                    <ul>
                        $forall Friend name age <- friends
                            <li>#{name} (#{age} years old)
                <footer>^{copyright}

        Lucius (CSS)

        section.blog {
            padding: 1em;
            border: 1px solid #000;
            h1 {
                color: #{headingColor};
                background-image: url(@{MyBackgroundR});
            }
        }

        Cassius (CSS)

        The following is equivalent to the Lucius example above.

        section.blog
            padding: 1em
            border: 1px solid #000
            h1
                color: #{headingColor}
                background-image: url(@{MyBackgroundR})
    * Не следует браться за это в лоб
    * Descend/Ascdend вариант
    *   indents
            bsd3
            основан на parsec
        indentparser
        indentation на hackage
    * Моя мысль: сложно парсить из-за разной топологии
    * Текущее решение: использовать layout-sensitive парсеры. Есть разные
        варианты, например, токены превратить в пару (токен, отступ) в грамматику можно добавлять предикаты >, >=, (*)
        Indentation-Sensitive Parsing for Parsec

    [TODO]

Функциональные парсеры
----------------------
    Монадические комбинаторы не позволяют
    Arrows

Сочетание комбинаторов и анализ
-------------------------------
    Peter Ljunglof, Pure Functional Parsing: an advanced tutorial.
    Использовали расширение к Haskell observable sharing (которое немного
    нарушает его pure)

    The resulting behaviour is similar to parser generators like Happy [28], in that
    we can make use of static typing and semantic actions at the same time as we
    can use an efficient parsing algorithm. Advantages over parser generators are
    that we can make use of Haskell to define new combinators to relieve us from
    cumbersome grammar writing, and that we don’t have to use another formalism
    than the syntax of standard Haskell. Some additional features of Happy, such
    as fixity declarations, can be handled by defining a tailor-made combinator that
    does the work for you. It is also straightforward to implement tests of the
    resulting grammar – e.g. to test whether it is in a certain class of languages.
    But there are disadvantages too. First, we have to use impure and experimental
    features of the Haskell implementations. In our opinion this is not a severe draw-
    back, since we use the features in a well-behaved way. Second, since we convert
    the parsers to context-free grammars we can only use context-free combinators.
    Third, we must be careful when we define parametric parsers, such as the many
    combinator, to avoid falling into non-termination while collecting the grammar.
    A final disadvantage is that we still have to go through an intermediate parse
    result, even though it is hidden from the user. This might lead to inefficiences
    when calculating the semantic actions.


Инкрементальная генерация парсеров
----------------------------------
        Полезно для:
    * Интерактивного редактирования грамматики
    * Пользовательский синтаксис (Haskell, Agda, Coq)
    Однако, видимо, почти нигде не используется?
    *  J. Heering, P. Klint, J. Rekers,  Incremental Generation of Parsers


Парсинг с использование производных
-----------------------------------
     «I'm against parsing tools that don't protect the user from surprises.
    LALR(1) generators are out because they don't accept all grammars. »

    «Alan Perlis once remarked, "for every polynomial-time algorithm you have,
    there is an exponential algorithm that I would rather run." »

Fuzzy парсинг
-------------
    WIP

Литература
----------
    WIP
